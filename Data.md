# Datasets

### Topic 1:Image Captioning

##### 1. MS COCO Dataset: 

The MS COCO dataset (Lin et al., 2014)  is a dataset used for image recognition, image detection, image segmentation and image captioning. For each image in the dataset, five different descriptions are provided. This dataset contains 91 objects types that would be easily recognizable. In the dataset, more than 300,000 images and a total of 2.5 million labeled instances are included. 

**Microsoft Coco: Common objects in context.** *Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick.* European conference on computer vision. Springer, Cham, 2014. [[paper]](https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48)



##### 2. Flickr30K Dataset:

The Flickr30k (Plummer et al., 2015)  is a datasets contains 31,000 images collected from Flickr, together with 5 image descriptions provided by human annotators. It's a benchmark dataset for image retrieval, cross-modal retrieval, zero-shot cross-modal retrieval and image captioning.

**Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.** *Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.* Proceedings of the IEEE international conference on computer vision. 2015.[[paper]](https://openaccess.thecvf.com/content_iccv_2015/html/Plummer_Flickr30k_Entities_Collecting_ICCV_2015_paper.html)



##### 3. Flickr8K Dataset:

The Flickr8k (Hodosh et al., 2013)  is a datasets consisting of 8,000 images collected from Flickr, and  5 captions for each image. It's a benchmark dataset for image retrieval, cross-modal retrieval, zero-shot cross-modal retrieval and image captioning. This dataset is small in size and the modal can be trained faster and easier.

**Framing image description as a ranking task: Data, models and evaluation metrics.** *Micah Hodosh, Peter Young, and Julia Hockenmaier.* Journal of Artificial Intelligence Research 47 (2013), 853–899. 2013. [[paper]](https://www.jair.org/index.php/jair/article/view/10833)



### Topic 2: Fake News Detection

##### 1. Weibo Dataset: 



### Topic 3: Visually Rich Document Understanding

##### 1. FUNSD Dataset:

FUNSD is a small dataset comprising of 199 real, fully annotated, scanned forms. FUNSD contains 31485 words, 9707 semantic entities, and 5304 relations, and aims at extracting and structuring the textual content of forms. This dataset can be used for text detection, optical character recognition, spatial layout analysis, and entity labeling/linking. 

**FUNSD: A dataset for form understanding in noisy scanned documents.** *Jaume G, Ekenel H K, Thiran J P.* International Conference on Document Analysis and Recognition Workshops (ICDARW). IEEE, 2019, 2: 1-6.  [[paper]](https://arxiv.org/abs/1905.13538)



##### 2. SROIE Dataset:

SROIE is consists of a dataset with 1000 whole scanned receipt images and annotations for the competition on scanned receipts OCR and key information extraction (SROIE).

**Icdar2019 competition on scanned receipt ocr and information extraction.** *Huang Z, Chen K, He J, et al.* International Conference on Document Analysis and Recognition (ICDAR). IEEE, 2019: 1516-1520. [[paper]](https://ieeexplore.ieee.org/abstract/document/8977955)





### Topic 4: Sentiment Analysis

##### 1. CMU-MOSEI Dataset:

CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset is the largest dataset of multimodal sentiment analysis and emotion recognition to date.  CMU-MOSEI is built in 2018, and contains 23,453 annotated viedo segments from 1000 distinct online YouTube and 250 topics.  Each sentence is annotated for sentiment on a [-3,3] Likert scale of: [−3: highly negative, −2 negative, −1 weakly negative, 0 neutral, +1 weakly positive, +2 positive, +3 highly positive].  This dataset will be useful to analyze the multimodal fusion in sentiment analysis and emotion recognition.



**Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph.** *Zadeh A, Pu P.* Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers). 2018.[[paper]](https://par.nsf.gov/servlets/purl/10081555)



### Topic 5: Visual Question Answering

##### 1. VQA:

VQA(Visual Question Answering) is a large dataset containing more than 250,000 images with at least 3 questions per image, and 10 ground truth answers per question. VQA v1. provides 6,141,630 ground truth answers and 1,842,489 plausible answers additionally which makes harder for the model to answer the questions correctly.  VAQ-cp(Visual Question Answering under Changing Priors) is the splits of the VQA v1 and VQA v2 datasets.

 **Vqa: Visual question answering.** *Antol S, Agrawal A, Lu J, et al.* Proceedings of the IEEE international conference on computer vision. 2015: 2425-2433.[[paper]](https://openaccess.thecvf.com/content_iccv_2015/html/Antol_VQA_Visual_Question_ICCV_2015_paper.html)

**C-vqa: A compositional split of the visual question answering (vqa) v1. 0 dataset.** *Agrawal A, Kembhavi A, Batra D, et al.* arXiv preprint arXiv:1704.08243, 2017.[[paper]](https://arxiv.org/abs/1704.08243)

**Don't just assume; look and answer: Overcoming priors for visual question answering.** *Agrawal A, Batra D, Parikh D, et al.* Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 4971-4980. [[paper]](https://openaccess.thecvf.com/content_cvpr_2018/html/Agrawal_Dont_Just_Assume_CVPR_2018_paper.html)



### Topic 6: Event Section

##### 1. CrisisMMD:

CrisisMMD is a large multi-modal dataset from natural disasters collected from Twitter including earthquakes, hurricanes, wildfires, and floods that happened in the year 2017 across different parts of the World. It has three types of annotations. The first one is "informative" or "Not informative", which determines whether it is useful for humanitarian aid. The second type is "Humanitarian Categories", such as infrastructure and utility damage, vehicle damage, and so on. The last type is "Damage Severity Assessment", which describes the severity of damage.

**Crisismmd: Multimodal twitter datasets from natural disasters.** *Alam F, Ofli F, Imran M.* Twelfth international AAAI conference on web and social media. 2018. [[paper]](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewPaper/17816)



### Topic 7: Information Retrieval

##### 1. MIMIC:

MIMIC(Medical Information Mart for Intensive Care)  is a publicly available dataset developed by the Laboratory for Computational Physiology that comprises deidentified health data associated with thousands of intensive care unit admissions. Currently, it has three versions: MIMIC-II、MIMIC-III、MIMIC-IV. MIMIC-III contains information on 53423 different hospitalized adult patients admitted to critical care units from 2001 to 2012, such as the patient's gender, height, and other basic information, such as blood routine, liver function and other hosipital test data, as well as the patient's medication information.

**MIMIC-III, a freely accessible critical care database.** *Johnson A E W, Pollard T J, Shen L, et al.* Scientific data, 2016, 3(1): 1-9. [[paper]](
