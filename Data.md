# Datasets

### Topic 1:Image Captioning

##### 1. MS COCO Dataset: 

The MS COCO dataset (Lin et al., 2014)  is a dataset used for image recognition, image detection, image segmentation and image captioning. For each image in the dataset, five different descriptions are provided. This dataset contains 91 objects types that would be easily recognizable. In the dataset, more than 300,000 images and a total of 2.5 million labeled instances are included. 

**Microsoft Coco: Common objects in context.** *Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick.* European conference on computer vision. Springer, Cham, 2014. [[paper]](https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48)



##### 2. Flickr30K Dataset:

The Flickr30k (Plummer et al., 2015)  is a datasets contains 31,000 images collected from Flickr, together with 5 image descriptions provided by human annotators. It's a benchmark dataset for image retrieval, cross-modal retrieval, zero-shot cross-modal retrieval and image captioning.

**Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.** *Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.* Proceedings of the IEEE international conference on computer vision. 2015.[[paper]](https://openaccess.thecvf.com/content_iccv_2015/html/Plummer_Flickr30k_Entities_Collecting_ICCV_2015_paper.html)



##### 3. Flickr8K Dataset:

The Flickr8k (Hodosh et al., 2013)  is a datasets consisting of 8,000 images collected from Flickr, and  5 captions for each image. It's a benchmark dataset for image retrieval, cross-modal retrieval, zero-shot cross-modal retrieval and image captioning. This dataset is small in size and the modal can be trained faster and easier.

**Framing image description as a ranking task: Data, models and evaluation metrics.** *Micah Hodosh, Peter Young, and Julia Hockenmaier.* Journal of Artificial Intelligence Research 47 (2013), 853–899. 2013. [[paper]](https://www.jair.org/index.php/jair/article/view/10833)



### Topic 2: Fake News Detection

##### 1. Weibo Dataset: 



### Topic 3: Visually Rich Document Understanding

##### 1. FUNSD Dataset:

FUNSD is a small dataset comprising of 199 real, fully annotated, scanned forms. FUNSD contains 31485 words, 9707 semantic entities, and 5304 relations, and aims at extracting and structuring the textual content of forms. This dataset can be used for text detection, optical character recognition, spatial layout analysis, and entity labeling/linking. 

**FUNSD: A dataset for form understanding in noisy scanned documents.** *Jaume G, Ekenel H K, Thiran J P.* International Conference on Document Analysis and Recognition Workshops (ICDARW). IEEE, 2019, 2: 1-6.  [[paper]](https://arxiv.org/abs/1905.13538)



##### 2. SROIE Dataset:

SROIE is consists of a dataset with 1000 whole scanned receipt images and annotations for the competition on scanned receipts OCR and key information extraction (SROIE).

**Icdar2019 competition on scanned receipt ocr and information extraction.** *Huang Z, Chen K, He J, et al.* International Conference on Document Analysis and Recognition (ICDAR). IEEE, 2019: 1516-1520. [[paper]](https://ieeexplore.ieee.org/abstract/document/8977955)





### Topic 4: Sentiment Analysis

##### 1. CMU-MOSEI Dataset:

CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset is the largest dataset of multimodal sentiment analysis and emotion recognition to date.  CMU-MOSEI is built in 2018, and contains 23,453 annotated viedo segments from 1000 distinct online YouTube and 250 topics.  Each sentence is annotated for sentiment on a [-3,3] Likert scale of: [−3: highly negative, −2 negative, −1 weakly negative, 0 neutral, +1 weakly positive, +2 positive, +3 highly positive].  This dataset will be useful to analyze the multimodal fusion in sentiment analysis and emotion recognition.



**Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph.** *Zadeh A, Pu P.* Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers). 2018.[[paper]](https://par.nsf.gov/servlets/purl/10081555)



### Topic 5: Visual Question Answering

##### 1. VQA:

VQA(Visual Question Answering) is a large dataset containing more than 250,000 images with at least 3 questions per image, and 10 ground truth answers per question. VQA v1. provides 6,141,630 ground truth answers and 1,842,489 plausible answers additionally which makes harder for the model to answer the questions correctly.  VAQ-cp(Visual Question Answering under Changing Priors) is the splits of the VQA v1 and VQA v2 datasets.

 **Vqa: Visual question answering.** *Antol S, Agrawal A, Lu J, et al.* Proceedings of the IEEE international conference on computer vision. 2015: 2425-2433.[[paper]](https://openaccess.thecvf.com/content_iccv_2015/html/Antol_VQA_Visual_Question_ICCV_2015_paper.html)

**C-vqa: A compositional split of the visual question answering (vqa) v1. 0 dataset.** *Agrawal A, Kembhavi A, Batra D, et al.* arXiv preprint arXiv:1704.08243, 2017.[[paper]](https://arxiv.org/abs/1704.08243)

**Don't just assume; look and answer: Overcoming priors for visual question answering.** *Agrawal A, Batra D, Parikh D, et al.* Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 4971-4980. [[paper]](https://openaccess.thecvf.com/content_cvpr_2018/html/Agrawal_Dont_Just_Assume_CVPR_2018_paper.html)



### Topic 6: Event Section

##### 1. CrisisMMD:

CrisisMMD is a large multi-modal dataset from natural disasters collected from Twitter including earthquakes, hurricanes, wildfires, and floods that happened in the year 2017 across different parts of the World. It has three types of annotations. The first one is "informative" or "Not informative", which determines whether it is useful for humanitarian aid. The second type is "Humanitarian Categories", such as infrastructure and utility damage, vehicle damage, and so on. The last type is "Damage Severity Assessment", which describes the severity of damage.

**Crisismmd: Multimodal twitter datasets from natural disasters.** *Alam F, Ofli F, Imran M.* Twelfth international AAAI conference on web and social media. 2018. [[paper]](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/viewPaper/17816)



### Topic 7: Information Retrieval

##### 1. MIMIC:

MIMIC(Medical Information Mart for Intensive Care)  is a publicly available dataset developed by the Laboratory for Computational Physiology that comprises deidentified health data associated with thousands of intensive care unit admissions. Currently, it has three versions: MIMIC-II、MIMIC-III、MIMIC-IV. MIMIC-III contains information on 53423 different hospitalized adult patients admitted to critical care units from 2001 to 2012, such as the patient's gender, height, and other basic information, such as blood routine, liver function and other hosipital test data, as well as the patient's medication information.

**MIMIC-III, a freely accessible critical care database.** *Johnson A E W, Pollard T J, Shen L, et al.* Scientific data, 2016, 3(1): 1-9. [[paper]](



### 8.Fashion-200k

Fashion200k contains 200K fashion images, and each image comes with a compact attribute-like product description.

[[paper]][https://www.kaggle.com/datasets/mayukh18/fashion200k-dataset]

https://openaccess.thecvf.com/content_CVPR_2019/html/Vo_Composing_Text_and_Image_for_Image_Retrieval_-_an_Empirical_CVPR_2019_paper.html

### 9.MIT-Stata Center

MIT-Stata Center is a multimodal dataset containing vision(stereo and RGB-D), laser and proprioceptive data. This dataset comprises over 2.3 TB, 38 h and 42 km. This dataset also includes ground-truth position estimates of the robot at every instance. This is a very useful dataset for robotic mapping and computer vision research.

[[paper]][https://journals.sagepub.com/doi/abs/10.1177/0278364913509035]

### 10. NYU Depth v1

The NYU-Depth dataset consists of video sequences recorded by the RGB and Depth cameras from the Microsoft Kinect. NYU-Depth v1 provides about 4GB of labeled data and about 90GB of raw data. NYU-Depth v1 includes 64 different indoor scenes and 7 scene types. NYU-Depth v2 includes 464 different indoor scenes and 26 scene types.

[[paper]][https://cs.nyu.edu/~silberman/datasets/nyu_depth_v1.html]

https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html



### 11.SKIG

The Shefeld Kinect Gesture (SKIG) is a gesture dataset containing 2160 hand gesture sequences. These hand gestures can be classified into 10 categories:  circle (clockwise), triangle (anti-clockwise), up-down, right-left, wave, "Z", cross, come-here, turn-around, and pat. The sequences are recorded under 3 different backgrounds and 2 illumination conditions which provide diversity.

[[paper]][https://mldta.com/dataset/shefeld-kinect-gesture-skig-dataset/]



### 12.GoodNews

GoodNews is a large dataset containing 466,000 images with captions, headlines, and text articles. However, different from datasets like MSCOCO or Flicker8k, GoodNews includes a single ground truth caption per image. GoodNews captions are written by expert journals, are longer on average than generic captioning datasets, meaning that these captions are more descriptive.

**Ali Furkan Biten, Lluis Gomez, Marcal Rusinol, Dimosthenis Karatzas**; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 12466-12475 [[paper]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Biten_Good_News_Everyone_Context_Driven_Entity-Aware_Captioning_for_News_Images_CVPR_2019_paper.pdf)

### 13.MSR-VTT

MSR-VTT is a large-scale video description dataset containing 10K web video clips with 38.7 hours and 200K clip-sentence pairs in total. MSR-VTTwas created from 257  popular queries from a commercial video search engine. Each clip in MSR-VTT is annotated with approximately 20 natural sentences. This dataset is presented for video understanding.

 [[paper]](https://openaccess.thecvf.com/content_cvpr_2016/html/Xu_MSR-VTT_A_Large_CVPR_2016_paper.html)

### 14.MSVD-QA 

The Microsoft Research Video Description Corpus (MSVD) dataset contains 122K descriptions of 2089 short video clips(usually less than 10 seconds). The MSVD dataset consists of different language descriptions, such as English, Hindi, Romanian, Slovene, etc. MSVD-QA is a benchmark dataset for video retrieval, visual question answering, and video captioning.

 [[paper]](https://www.cs.utexas.edu/users/ml/clamp/videoDescription/)

### 15. TGIF-QA

TGIF-QA is a large-scale dataset containing 103,919 QA pairs collected from 56,720 animated GIFs. These GIFs are from the TGIF dataset. The TGIF dataset is based on GIFS data as GIFs have a concise format and cohesive storytelling nature. TGIF-QA can be used for visual question answering research.

 [[paper]](https://openaccess.thecvf.com/content_cvpr_2017/html/Jang_TGIF-QA_Toward_Spatio-Temporal_CVPR_2017_paper.html)

### 16.EQA-v1

EQA(Embodied Question Answering)v1.0 is a dataset containing 9,000 questions from 774 environments. The visual questions and answers in this dataset are grounded in House3D. EQA-v1 contains location, color, and place preposition questions. In order to answer the question, EQA has an agent to explore the house, and then the agent is trained to answer the question.

 [[paper]](https://embodiedqa.org/data)

https://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Multi-Target_Embodied_Question_Answering_CVPR_2019_paper.html

### 17.VideoNavQA

VideoNavQA is also a dataset used to study the EQA task. VideoNavQA contains 28 questions belonging to 8 categories with 70 possible answers. The complexity of the questions in VideoNavQA far exceeds that of similar tasks that use generation methods that extract ground truth information from video to generate questions.

 [[paper]](https://bmvc2019.org/wp-content/uploads/papers/1125-paper.pdf)

### 18.TDIUC

Task Directed Image Understanding Challenge (TDIUC) is a dataset containing 167,437 Images and 1,654,167 question-answer pairs. TDIUC divides VQA into 12 constituent tasks, which makes it easier to measure and compare the performance of VQA algorithms. The 12 different question-types are grouped according to these tasks.

 [[paper]][https://kushalkafle.com/projects/tdiuc.html]

### 19.nuScenes

nuScenes is a public large-scale dataset for autonomous driving dataset with 3d object annotations. It is also a multimodal dataset. nuScenes provides 1.4 million camera images, 1500h of driving data from 4 cities (Boston, Pittsburgh, Las Vegas and Singapore),  sensor data released for 150h (5x LIDAR, 8x camera, IMU, GPS), detailed map information, 1.4M 3D bounding boxes manually annotated for 23 object classes, etc. nuScenes can be used for intelligent agent research.

nuImages is a large-scale autonomous driving dataset with image-level 2d annotations. It has 93k video clips of 6s each, 93k annotated and 1.1M un-annotated images. 

[[paper]][https://www.nuscenes.org/]

